{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instsall tar2tf module\n",
    "! pip3 install ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIS Buckets (1)\n",
      "  tar-bucket\n"
     ]
    }
   ],
   "source": [
    "# list available ais buckets\n",
    "!ais ls ais://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME\t\t SIZE\t\t VERSION\t \n",
      "train-0.tar\t17.48MiB\t1\t\t\n",
      "train-1.tar\t17.48MiB\t1\t\t\n",
      "train-10.tar\t17.48MiB\t1\t\t\n",
      "train-11.tar\t17.48MiB\t1\t\t\n",
      "train-12.tar\t17.48MiB\t1\t\t\n",
      "train-13.tar\t17.48MiB\t1\t\t\n",
      "train-14.tar\t17.48MiB\t1\t\t\n",
      "train-15.tar\t17.48MiB\t1\t\t\n",
      "train-16.tar\t17.48MiB\t1\t\t\n",
      "train-17.tar\t17.48MiB\t1\t\t\n",
      "train-18.tar\t17.48MiB\t1\t\t\n",
      "train-19.tar\t17.48MiB\t1\t\t\n",
      "train-2.tar\t17.48MiB\t1\t\t\n",
      "train-20.tar\t17.48MiB\t1\t\t\n",
      "train-3.tar\t17.48MiB\t1\t\t\n",
      "train-4.tar\t17.48MiB\t1\t\t\n",
      "train-5.tar\t17.48MiB\t1\t\t\n",
      "train-6.tar\t17.48MiB\t1\t\t\n",
      "train-7.tar\t17.48MiB\t1\t\t\n",
      "train-8.tar\t17.48MiB\t1\t\t\n",
      "train-9.tar\t17.48MiB\t1\t\t\n"
     ]
    }
   ],
   "source": [
    "# list contents of your bucket\n",
    "!ais ls ais://tar-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from client.ais_tar2tf import AisDataset\n",
    "from client.ais_tar2tf.ops import Select, Decode, Convert, Resize, Rotate, Func\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# ADJUST AisDataset PARAMETERS BELOW\n",
    "\n",
    "BUCKET_NAME = \"tar-bucket\"\n",
    "PROXY_URL = \"http://localhost:8080\"\n",
    "\n",
    "INPUT_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AisDataset.\n",
    "# tar records will be transformed according [Decode(\"jpg\"), Rotate(\"jpg\"), Resize(\"jpg\", (224, 224)) operations,\n",
    "# meaning that bytes under \"jpg\" in tar-record will be decoded as an image, Rotated by random angle and then Resized to (224, 224)\n",
    "# Datapoints will be constructed from \"jpg\" and \"cls\" tar records entries\n",
    "conversions = [Decode(\"jpg\"), Rotate(\"jpg\"), Resize(\"jpg\", (224, 224))]\n",
    "selections = [\"jpg\", \"cls\"]\n",
    "ais = AisDataset(BUCKET_NAME, PROXY_URL, conversions, selections)\n",
    "\n",
    "# Prepare your bucket first with Gavin's tars (gsutil ls gs://lpr-gtc2020)\n",
    "# Remote conversions and selections execution by default\n",
    "# Datasets will be prepared from \"train-{0..5}.tar\" files, records of each tar file will be shuffled within a scope of a tar file\n",
    "# Prefetches BATCH_SIZE, limits dataset first BATCH_SIZE * 5 elements, caches them, repeats forever and creates batches from infinite dataset\n",
    "train_dataset = ais.load(\"train-{0..5}.tar\", num_workers=5, shuffle_tar=True, output_shapes=(tf.TensorShape(INPUT_SHAPE), tf.TensorShape([None]))\n",
    ").take(BATCH_SIZE * 5).cache().repeat().batch(BATCH_SIZE)\n",
    "test_dataset = ais.load(\"train-{6..10}.tar\", num_workers=5, output_shapes=(tf.TensorShape(INPUT_SHAPE), tf.TensorShape([None]))\n",
    ").take(BATCH_SIZE).cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PART BELOW\n",
    "inputs = keras.Input(shape=(\n",
    "    224,\n",
    "    224,\n",
    "    3,\n",
    "), name=\"images\")\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-4), loss=keras.losses.mean_squared_error, metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=BATCH_SIZE)\n",
    "result = model.evaluate(test_dataset)\n",
    "print(dict(zip(model.metrics_names, result)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}